## Adversarial Machine learning:

Adversarial machine learning is a machine learning method that aims to trick machine learning models by providing deceptive input. Hence, it includes both the generation and detection of adversarial examples, which are inputs specially created to deceive classifiers.

**Difference between adversarial whitebox and blackbox attacks:**

A *whitebox attack* is a scenario where the attacker has complete access to the target model, including the model's architecture and its parameters.

A *blackbox attack* is a scenario where an attacker has no access to the model and can only observe the outputs of the targeted model.

Adversarial machine learning is the field that studies a class of attacks that aims to deteriorate the performance of classifiers on specific tasks. Adversarial attacks can be mainly classifier into the following categories:
* Poisoning attacks 
* Evasion attacks 
* Model extraction attacks 
  
**Poisoning attacks:**  
The attacker influences the training data or its labels to cause the model to underperform during deployment. Hence, poisoning is essentially adversarial contamination of training data.

![](https://viso.ai/wp-content/uploads/2021/06/adversarial-machine-learning-poisoning-attack.jpg)

**Evasion attacks:**  
Evasion attacks are the most prevalent and most researched types of attacks. The attacker manipulates the data during deployment to deceive previously trained classifiers. 
Since they are performed during the deployment phase, they are the most practical types of attacks and the most used attacks on intrusion and malware scenarios.

**Model extraction attacks:**   
Model stealing or model extraction involves an attacker probing a black box machine learning system in order to either reconstruct the model or extract the data it was traine on. This is especially significant when either the training data or the model itself is sensitive and confidential.

## Popular Adversarial Attacks:

* **Limited-Memory BFGS (L-BFGS) attack:**  
 The limited-memory Broyden-Fletcher-Goldfarb-shanno (L-BFGS) method is a non-linear gradient-based numerical optimization algorithm to minimize the number of pertubations added to images.

* **Fast Gradient Sign Method (FGSM):**     
A simple and fast gradient-based method is used to generate adversarial examples to minimize the maximum amount of perturbation added to images.

* **Jacobian-based Salience Map Attack (JSMA):**        
Unlike FGSM, the method uses feature selection to minimize the number of features modified while causing misclassification. Flat pertubations are added to features iteratively according to saliency value by decreasing order.